{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "163zyCyI3B8g53cFOutCu9DDEeulwvK2j",
      "authorship_tag": "ABX9TyNCQOVL7M1hN140poNeVR74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditichak22/nlp-rnn/blob/main/PostRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB_MuuS5KuR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bfeb03-ac14-4174-b596-73c49cd74552"
      },
      "source": [
        "# install necessary packages using pip\n",
        "!pip install keras numpy wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=e33727fc7036577d60321279b27be5f156847fe1b0d20a7924b7f9743f1baf43\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0GYsJU2hm16"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT37l9LMoGYx"
      },
      "source": [
        "def load_corpus(path):\n",
        "\n",
        "    # Check if the path is a directory.\n",
        "    if not os.path.isdir(path):\n",
        "        sys.exit(\"Input path is not a directory\")\n",
        "    corpusList = []\n",
        "    for filename in os.listdir(path):\n",
        "      filename = os.path.join(path, filename)\n",
        "      try:\n",
        "        reader = io.open(filename)\n",
        "        for line in reader:\n",
        "          textList = line.split()\n",
        "          newList = list(map(lambda x: tuple(x.split(\"/\")), textList))\n",
        "          if (len(newList) > 0):\n",
        "            corpusList.append(list(newList))\n",
        "      except IOError:\n",
        "        sys.exit(\"Cannot read file\")\n",
        "    return corpusList\n",
        "    \n",
        "\n",
        "# test the function here:\n",
        "path = \"drive/MyDrive/modified_brown\"\n",
        "data = load_corpus(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFJvfGCPois_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40191feb-e9e6-403a-ef01-b86b28bbe8dd"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "\n",
        "# Creates the dataset with train_X (words) and train_y (tag).\n",
        "def create_dataset(sentences):\n",
        "    # Defines the relevant lists.\n",
        "    train_X, train_y = list(), list()\n",
        "    tags = set()\n",
        "    vocab = set()\n",
        "    tags_dict = {}\n",
        "    vocab_dict = {}\n",
        "    for sentence in sentences:\n",
        "      for word in sentence:\n",
        "        vocab.add(word[0])\n",
        "        tags.add(word[1])\n",
        "    \n",
        "    for i,j in enumerate(vocab):\n",
        "      vocab_dict[j] = i + 1\n",
        "\n",
        "    for i,j in enumerate(tags):\n",
        "      tags_dict[j] = i + 1\n",
        "    \n",
        "\n",
        "    vocab_dict[\"PAD\"] = 0\n",
        "    tags_dict[\"PAD\"] = 0\n",
        "\n",
        "\n",
        "    for sentence in sentences:\n",
        "      vec_words = []\n",
        "      vec_tags = []\n",
        "      for word in sentence:\n",
        "        vec_words.append(vocab_dict[word[0]])\n",
        "        vec_tags.append(tags_dict[word[1]])\n",
        "      train_X.append(vec_words)\n",
        "      train_y.append(vec_tags)\n",
        "    \n",
        "\n",
        "    return np.array(train_X), np.array(train_y), len(vocab), len(tags), list(tags), vocab_dict, tags_dict\n",
        "\n",
        "\n",
        "# Test the function here\n",
        "# Call create_dataset()\n",
        "train_X, train_y, vocab_size, tag_size, tag_list, vocab_dict, tags_dict = create_dataset(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk0ZTHkvplxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c12764-986e-443f-dbfa-f0293ad559f3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences as pad\n",
        "\n",
        "\n",
        "# Pad the sequences with 0s to the max length.\n",
        "def pad_sequences(train_X, train_y):\n",
        "    # Use MAX_LENGTH to record length of longest sequence \n",
        "\n",
        "\n",
        "    MAX_LENGTH = len(max(train_X, key=len))\n",
        "    X_padded = pad(train_X, MAX_LENGTH, padding='post')\n",
        "    Y_padded = pad(train_y, MAX_LENGTH, padding='post')\n",
        "\n",
        "    print(Y_padded)\n",
        "    return X_padded, Y_padded, MAX_LENGTH\n",
        "\n",
        "# Test the function\n",
        "train_X, train_y, MAX_LENGTH = pad_sequences(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5 10  7 ...  0  0  0]\n",
            " [ 9  9  2 ...  0  0  0]\n",
            " [ 9  5  2 ...  0  0  0]\n",
            " ...\n",
            " [ 4  7  3 ...  0  0  0]\n",
            " [ 2  8  7 ...  0  0  0]\n",
            " [ 6  2  1 ...  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edy9gTV6qIhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05db673e-89b9-4313-8d7d-f81fdf4191e4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, Activation\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "\n",
        "# Define the Keras model.\n",
        "def define_model(MAX_LENGTH):  \n",
        "    \n",
        "    # Define 'model' here\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(vocab_dict), 128, input_length=MAX_LENGTH))\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(len(tags_dict))))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "    print (model.summary())\n",
        "    return model\n",
        "\n",
        "# Call the function here\n",
        "model = define_model(MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 180, 128)          7165952   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 180, 12)           6156      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 180, 12)           0         \n",
            "=================================================================\n",
            "Total params: 7,960,588\n",
            "Trainable params: 7,960,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0c2a7eUPQFi"
      },
      "source": [
        "# Returns the one-hot encoding of the sequence.\n",
        "from keras.utils.np_utils import to_categorical as categorical\n",
        "def to_categorical(train_y, categories):\n",
        "    return categorical(train_y, num_classes=categories)\n",
        "\n",
        "\n",
        "# Call the function as to_categorical(train_y, categories = len(tag2idx))\n",
        "\n",
        "train_y1 = to_categorical(train_y, len(tags_dict))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN-Roc_AORIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af9e330-6502-4a5d-c9ed-ea48c3b5bc4a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Trains the model.\n",
        "def train(model, train_X, train_y):\n",
        "\n",
        "    # Fit the data into the Keras model, through 40 passes (epochs) using model.fit()\n",
        "\n",
        "    model = model.fit(train_X, train_y, validation_split=0.2, batch_size=128, epochs=40)\n",
        "\n",
        "    # Return the model.\n",
        "    return model\n",
        "\n",
        "# call function here\n",
        "trained_model = train(model, train_X, train_y1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "359/359 [==============================] - 67s 163ms/step - loss: 0.1849 - accuracy: 0.9464 - val_loss: 0.0267 - val_accuracy: 0.9929\n",
            "Epoch 2/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0131 - val_accuracy: 0.9958\n",
            "Epoch 3/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0119 - val_accuracy: 0.9962\n",
            "Epoch 4/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0116 - val_accuracy: 0.9963\n",
            "Epoch 5/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0121 - val_accuracy: 0.9964\n",
            "Epoch 6/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0124 - val_accuracy: 0.9963\n",
            "Epoch 7/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0130 - val_accuracy: 0.9963\n",
            "Epoch 8/40\n",
            "359/359 [==============================] - 57s 159ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0137 - val_accuracy: 0.9963\n",
            "Epoch 9/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0153 - val_accuracy: 0.9960\n",
            "Epoch 10/40\n",
            "359/359 [==============================] - 57s 159ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0154 - val_accuracy: 0.9962\n",
            "Epoch 11/40\n",
            "359/359 [==============================] - 57s 160ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0164 - val_accuracy: 0.9961\n",
            "Epoch 12/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 7.9458e-04 - accuracy: 0.9998 - val_loss: 0.0179 - val_accuracy: 0.9960\n",
            "Epoch 13/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 6.0544e-04 - accuracy: 0.9998 - val_loss: 0.0205 - val_accuracy: 0.9959\n",
            "Epoch 14/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 4.5289e-04 - accuracy: 0.9999 - val_loss: 0.0197 - val_accuracy: 0.9959\n",
            "Epoch 15/40\n",
            "359/359 [==============================] - 57s 160ms/step - loss: 3.6872e-04 - accuracy: 0.9999 - val_loss: 0.0210 - val_accuracy: 0.9959\n",
            "Epoch 16/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.6940e-04 - accuracy: 0.9999 - val_loss: 0.0219 - val_accuracy: 0.9959\n",
            "Epoch 17/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.1386e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9958\n",
            "Epoch 18/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.9885e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9959\n",
            "Epoch 19/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 2.1401e-04 - accuracy: 0.9999 - val_loss: 0.0236 - val_accuracy: 0.9958\n",
            "Epoch 20/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.9559e-04 - accuracy: 0.9999 - val_loss: 0.0249 - val_accuracy: 0.9958\n",
            "Epoch 21/40\n",
            "359/359 [==============================] - 58s 160ms/step - loss: 1.5638e-04 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 0.9959\n",
            "Epoch 22/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.0019e-04 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9958\n",
            "Epoch 23/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 8.5066e-05 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 0.9958\n",
            "Epoch 24/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 8.7858e-05 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 0.9958\n",
            "Epoch 25/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.4221e-04 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 0.9956\n",
            "Epoch 26/40\n",
            "359/359 [==============================] - 57s 160ms/step - loss: 1.8819e-04 - accuracy: 0.9999 - val_loss: 0.0278 - val_accuracy: 0.9959\n",
            "Epoch 27/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.6191e-04 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 0.9957\n",
            "Epoch 28/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 1.0757e-04 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 0.9959\n",
            "Epoch 29/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 5.0756e-05 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 0.9958\n",
            "Epoch 30/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.9997e-05 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9959\n",
            "Epoch 31/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.2001e-05 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 0.9958\n",
            "Epoch 32/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.0553e-05 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9958\n",
            "Epoch 33/40\n",
            "359/359 [==============================] - 58s 162ms/step - loss: 1.5949e-05 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9958\n",
            "Epoch 34/40\n",
            "359/359 [==============================] - 58s 162ms/step - loss: 9.0518e-05 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9956\n",
            "Epoch 35/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 2.6119e-04 - accuracy: 0.9999 - val_loss: 0.0302 - val_accuracy: 0.9958\n",
            "Epoch 36/40\n",
            "359/359 [==============================] - 58s 162ms/step - loss: 1.2034e-04 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 0.9959\n",
            "Epoch 37/40\n",
            "359/359 [==============================] - 58s 162ms/step - loss: 5.4609e-05 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9959\n",
            "Epoch 38/40\n",
            "359/359 [==============================] - 58s 162ms/step - loss: 1.9361e-05 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 0.9959\n",
            "Epoch 39/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 8.9192e-06 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 0.9959\n",
            "Epoch 40/40\n",
            "359/359 [==============================] - 58s 161ms/step - loss: 7.5190e-06 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPGv0ZXjJsOC"
      },
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANPb-K98i0w8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c134cd23-9b2f-47fc-88d3-9668d806d470"
      },
      "source": [
        "# Test a sentence using the given model.\n",
        "def test(model, sentence):\n",
        "  sentenceList = sentence.split()\n",
        "  wordToInt = []\n",
        "  for word in sentenceList:\n",
        "    if (word in vocab_dict):\n",
        "      wordToInt.append(vocab_dict[word])\n",
        "    else:\n",
        "      wordToInt.append(vocab_dict[\"PAD\"])\n",
        "\n",
        "  testSentence = pad([wordToInt], MAX_LENGTH, padding='post')\n",
        "  predictions = model.predict(testSentence)\n",
        "  result = logits_to_tokens(predictions, {i: t for t, i in tags_dict.items()})\n",
        "  print(result[0][:len(sentenceList)])\n",
        "\n",
        "\n",
        "test(model, \"the secretariat is expected to race tomorrow .\")\n",
        "test(model, \"people continue to enquire the reason for the race for outer space .\")\n",
        "\n",
        "test(model, \"the planet jupiter and its moons are in effect a mini solar system .\")\n",
        "test(model, \"computers process programs accurately .\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DETERMINER', 'NOUN', 'VERB', 'VERB', 'PREPOSITION', 'VERB', 'NOUN', 'PUNCT']\n",
            "['NOUN', 'VERB', 'PREPOSITION', 'VERB', 'DETERMINER', 'NOUN', 'PREPOSITION', 'DETERMINER', 'NOUN', 'PREPOSITION', 'ADJECTIVE', 'NOUN', 'PUNCT']\n",
            "['DETERMINER', 'NOUN', 'ADJECTIVE', 'CONJUNCTION', 'PRONOUN', 'NOUN', 'VERB', 'PREPOSITION', 'NOUN', 'DETERMINER', 'ADJECTIVE', 'ADJECTIVE', 'NOUN', 'PUNCT']\n",
            "['VERB', 'NOUN', 'NOUN', 'ADVERB', 'PUNCT']\n"
          ]
        }
      ]
    }
  ]
}